# 02 - Cliente MCP com Interpreta√ß√£o Inteligente

## üéØ Objetivo

Gerar o arquivo `mcp_client.py` que implementa um cliente MCP com integra√ß√£o ao LLM, capaz de interpretar opera√ß√µes matem√°ticas solicitadas em linguagem natural e execut√°-las atrav√©s do servidor MCP.

## üìã Contexto

O cliente MCP conecta-se ao servidor via stdio, integra-se com um LLM (via API compat√≠vel com OpenAI) para interpretar as inten√ß√µes do usu√°rio, e executa tool calls apropriados. O sistema deve entender linguagem natural como "Eu tenho 25 ma√ß√£s e ganhei mais 13".

## üîß Especifica√ß√µes T√©cnicas

- **Framework:** MCP Client, OpenAI SDK
- **Comunica√ß√£o:** stdio com servidor MCP
- **LLM:** API compat√≠vel com OpenAI
- **Interface:** Chat interativo via terminal
- **Hist√≥rico:** Mant√©m contexto da conversa√ß√£o

---

## üí¨ PROMPT COMPLETO

```
Voc√™ √© um desenvolvedor Python especialista em AI Agents e Model Context Protocol (MCP).

TAREFA:
Crie um arquivo Python chamado "mcp_client.py" que implemente um cliente MCP com integra√ß√£o a um LLM para criar uma calculadora inteligente com interface de chat.

REQUISITOS FUNCIONAIS:

1. CONEX√ÉO COM SERVIDOR MCP:
   - Conectar ao servidor via stdio (subprocess)
   - Comando: python3, args: ["mcp_server.py"]
   - Usar StdioServerParameters e stdio_client
   - Inicializar sess√£o e obter lista de ferramentas dispon√≠veis

2. INTEGRA√á√ÉO COM LLM:
   - API Base URL: "https://api.xptoai.com.br/genai/grokcode/v1"
   - API Key: "chave de API para o servi√ßo de LLM"
   - Modelo: "grokcode"
   - Usar OpenAI SDK (compat√≠vel)

3. INTERPRETA√á√ÉO INTELIGENTE:
   O LLM deve ser instru√≠do (via system prompt) a:
   - Interpretar qual opera√ß√£o matem√°tica o usu√°rio deseja
   - Identificar os dois n√∫meros envolvidos na conversa
   - Chamar a ferramenta apropriada automaticamente
   - Apresentar resultados de forma amig√°vel

4. SYSTEM PROMPT (incluir no c√≥digo):
   """
   Voc√™ √© um assistente matem√°tico inteligente que ajuda usu√°rios a realizar opera√ß√µes matem√°ticas.
   Voc√™ tem acesso a 4 ferramentas: soma, subtra√ß√£o, multiplica√ß√£o e divis√£o.
   
   Seu objetivo √©:
   1. Interpretar qual opera√ß√£o matem√°tica o usu√°rio deseja realizar atrav√©s da conversa natural
   2. Identificar os dois n√∫meros que o usu√°rio quer usar
   3. Chamar a ferramenta apropriada com os n√∫meros corretos
   4. Apresentar o resultado de forma clara e amig√°vel
   
   Exemplos de interpreta√ß√£o:
   - 'Quanto √© 5 mais 3?' ‚Üí usar ferramenta 'soma' com 5 e 3
   - 'Subtraia 10 de 25' ‚Üí usar ferramenta 'subtracao' com 25 e 10
   - 'Multiplique 7 por 8' ‚Üí usar ferramenta 'multiplicacao' com 7 e 8
   - 'Divida 20 por 4' ‚Üí usar ferramenta 'divisao' com 20 e 4
   - '15 menos 7' ‚Üí usar ferramenta 'subtracao' com 15 e 7
   
   Se o usu√°rio pedir uma opera√ß√£o que n√£o est√° dispon√≠vel (como pot√™ncia, raiz quadrada, etc.),
   informe educadamente que apenas as 4 opera√ß√µes b√°sicas est√£o implementadas.
   
   Se houver erro na execu√ß√£o (como divis√£o por zero), explique o erro de forma educada e sugira uma alternativa.
   
   Seja conversacional, natural e prestativo na intera√ß√£o.
   """

5. CLASSE MCPChatClient:
   M√©todos necess√°rios:
   - __init__(): Inicializar OpenAI client, session, tools, hist√≥rico
   - connect_to_server(): Conectar ao servidor MCP via stdio
   - format_tools_for_openai(): Converter ferramentas MCP para formato OpenAI
   - call_mcp_tool(tool_name, arguments): Executar ferramenta no servidor
   - chat(user_message): Processar mensagem e retornar resposta
   - close(): Fechar conex√µes

6. FLUXO DO M√âTODO chat():
   a. Adicionar mensagem do usu√°rio ao hist√≥rico
   b. Preparar mensagens (system prompt + hist√≥rico)
   c. Chamar LLM com ferramentas dispon√≠veis
   d. Se LLM retornar tool_calls:
      - Executar cada tool call no servidor MCP
      - Adicionar resultados ao hist√≥rico
      - Fazer nova chamada ao LLM com os resultados
      - Retornar resposta final generativa
   e. Se n√£o houver tool_calls:
      - Retornar resposta textual diretamente

7. FEEDBACK VISUAL:
   Ao executar ferramentas, imprimir:
   ```
   üîß Executando ferramenta: nome_da_ferramenta
      Argumentos: {argumentos}
      ‚úì Resultado: valor    (se sucesso)
      ‚ö†Ô∏è  Erro: mensagem     (se erro)
   ```

8. INTERFACE DE CHAT:
   - Fun√ß√£o main() com loop interativo
   - Prompt: "üë§ Voc√™: "
   - Resposta: "ü§ñ Assistente: "
   - Comando "sair" para encerrar
   - Tratamento de KeyboardInterrupt (Ctrl+C)

9. GERENCIAMENTO DE CONTEXTO:
   - Usar context managers ass√≠ncronos corretamente
   - stdio_context = stdio_client(server_params)
   - await stdio_context.__aenter__()
   - await stdio_context.__aexit__() no close()

REQUISITOS T√âCNICOS:

- Imports: asyncio, json, typing.Optional, mcp, mcp.client.stdio, openai
- Usar async/await para todas opera√ß√µes ass√≠ncronas
- Manter hist√≥rico de conversa√ß√£o (lista de dicts)
- Shebang: #!/usr/bin/env python3
- Docstrings em portugu√™s

ESTRUTURA DO C√ìDIGO:

1. Imports e constantes (API_BASE_URL, API_KEY, MODEL_NAME)
2. Classe MCPChatClient com todos os m√©todos
3. Fun√ß√£o async main() com loop de chat
4. Bloco if __name__ == "__main__"

EXEMPLO DE CONVERS√ÉO DE FERRAMENTAS:
```python
def format_tools_for_openai(self) -> list:
    openai_tools = []
    for tool in self.available_tools:
        openai_tools.append({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema
            }
        })
    return openai_tools
```

Por favor, gere o c√≥digo completo do arquivo mcp_client.py seguindo todas estas especifica√ß√µes.
```

---

## ‚úÖ Resultado Esperado

Voc√™ deve receber um arquivo Python completo (~290 linhas) com:

**Estrutura:**
```python
#!/usr/bin/env python3
"""
Cliente MCP que interage com o usu√°rio via chat usando LLM...
"""

import asyncio
import json
from typing import Optional
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from openai import OpenAI

# Configura√ß√£o do LLM
API_BASE_URL = "https://api.xptoai.com.br/genai/grokcode/v1"
API_KEY = "biasb986lk657fsdv6d3543vs5b65s7v373sd321vsdv4sdv34bv3f4hb5f4j6mn546tu"
MODEL_NAME = "grok-2-1212"

class MCPChatClient:
    # M√©todos...

async def main():
    # Loop de chat...

if __name__ == "__main__":
    asyncio.run(main())
```

**Caracter√≠sticas:**
- ‚úÖ Classe MCPChatClient completa
- ‚úÖ Conex√£o com servidor MCP via stdio
- ‚úÖ Integra√ß√£o com OpenAI SDK
- ‚úÖ System prompt para interpreta√ß√£o inteligente
- ‚úÖ Convers√£o de ferramentas MCP para formato OpenAI
- ‚úÖ Execu√ß√£o de tool calls
- ‚úÖ Feedback visual (‚úì e ‚ö†Ô∏è)
- ‚úÖ Loop de chat interativo
- ‚úÖ Tratamento de erros

---

## üß™ Como Validar

Salve o c√≥digo gerado em `mcp_project/mcp_client.py` e verifique:

```bash
# 1. Verificar sintaxe Python
python -m py_compile mcp_client.py

# 2. Verificar imports
python -c "import ast; ast.parse(open('mcp_client.py').read())"

# 3. Contar linhas
wc -l mcp_client.py

# 4. Verificar se constantes est√£o definidas
grep -E "API_BASE_URL|API_KEY|MODEL_NAME" mcp_client.py
```

**Sa√≠da esperada:**
```
294 mcp_client.py
```

**Verifica√ß√£o manual:**
- [ ] Arquivo tem shebang `#!/usr/bin/env python3`
- [ ] Constantes API_BASE_URL, API_KEY, MODEL_NAME definidas
- [ ] Classe MCPChatClient presente
- [ ] M√©todo connect_to_server() implementado
- [ ] M√©todo chat() com l√≥gica de tool calls
- [ ] System prompt detalhado inclu√≠do
- [ ] Fun√ß√£o main() com loop interativo
- [ ] Feedback visual com emojis (üîß, ‚úì, ‚ö†Ô∏è)

---

## üìù Notas

- O cliente inicia o servidor automaticamente via subprocess
- A API key est√° hardcoded (em produ√ß√£o, use vari√°veis de ambiente)
- O hist√≥rico de conversa√ß√£o permite contexto entre mensagens
- O LLM decide quando chamar ferramentas baseado no system prompt

---

## üîß Troubleshooting

**Erro: "Module 'openai' not found"**
- Solu√ß√£o: Instale as depend√™ncias (ser√° feito no prompt 04)

**Erro: "Connection refused" ao conectar servidor**
- Verifique se mcp_server.py existe no mesmo diret√≥rio
- Confirme que o comando no StdioServerParameters est√° correto

**LLM n√£o chama ferramentas:**
- Verifique se o system prompt est√° correto
- Confirme que format_tools_for_openai() retorna formato v√°lido

---

## ‚û°Ô∏è Pr√≥ximo Passo

Ap√≥s validar o cliente, prossiga para: **`03-TESTS.md`**
